{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76763a6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33576/3297002015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mMaxPool2D\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Import necessary  libraries\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale\n",
    "import librosa.display\n",
    "import os\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "#glob used to return all file paths that match a specific pattern\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c08a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\n",
      "     |████▎                           | 65.0 MB 2.2 MB/s eta 0:03:14"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = [-6, 0, 6]\n",
    "kinds = ['normal', 'abnormal']\n",
    "file_name = ['00','02','04','06']\n",
    "def name_path(name):r\n",
    "    paths = []\n",
    "\n",
    "    for db in dbs:\n",
    "        for i in range(4):\n",
    "            paths.append(f'/Users/yusufakcakaya/Downloads/{db}_dB_valve/id_0{i*2}/{name}')\n",
    "    return paths\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c732c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_names = {'normal': name_path(kinds[0]),'abnormal': name_path(kinds[1])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189dfd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d34f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_png(files,name_dir,name_file):\n",
    "        \n",
    "    files = glob(files + '/*.wav')\n",
    "    \n",
    "    \n",
    "    for file in range(101):\n",
    "        data, sr = librosa.load(files[file])\n",
    "        data = scale(data)\n",
    "       \n",
    "        countstr = str(file)\n",
    "\n",
    "        melspec = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128)\n",
    "           \n",
    "        log_melspec = librosa.power_to_db(melspec, ref=np.max)  \n",
    "        librosa.display.specshow(log_melspec, sr=sr)\n",
    "            \n",
    "        if file < 80:\n",
    "\n",
    "            # save to png\n",
    "            directory = name_dir\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "        \n",
    "            png_number= name_file + countstr  \n",
    "\n",
    "\n",
    "            plt.savefig(directory + '/' + (png_number) + '.png')\n",
    "        elif file > 80:\n",
    "            \n",
    "             # save to png\n",
    "            directory = name_dir + '_validation'\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "        \n",
    "            png_number= name_file + countstr  \n",
    "\n",
    "\n",
    "            plt.savefig(directory + '/' + (png_number) + '.png')\n",
    "            \n",
    "    \n",
    "    return 0\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ['-6','0','6']\n",
    "kinds = ['normal', 'abnormal']\n",
    "file_name = ['00','02','04','06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ffec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = 0\n",
    "#for b in kinds:\n",
    "#    for i in db:\n",
    "#        for a in file_name:\n",
    "#            png_name = i+a\n",
    "#            \n",
    "#            to_png(file_names[b][index],b,png_name)\n",
    "            \n",
    "#            index += 1\n",
    "#            if index == 12:\n",
    "#                index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49026f1d",
   "metadata": {},
   "source": [
    "Define the path to our data. Let’s define a function called get_data() that makes it easier for us to create our train and validation dataset. We define the two labels ‘normal’ and ‘abnormal’ that we will use. We use the Opencv imread function to read the images in the RGB format and resize the images to our desired width and height in this case both being 224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08478fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"normal\", \"abnorm\"]\n",
    "img_size = 224\n",
    "\n",
    "def get_data(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)   # create path \n",
    "        class_num = labels.index(label)        # get the classification  (0 or a 1). 0=Normal 1=Abnormal\n",
    "        for img in os.listdir(path):           # iterate over each image per two of them\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch our train and validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = get_data(\"valve\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[:,0], data[:,1], test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0,0].shape)\n",
    "print(test[0,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train len',len(train))\n",
    "print('Test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for feature, label in train:\n",
    "  x_train.append(feature)\n",
    "  y_train.append(label)\n",
    "\n",
    "for feature, label in test:\n",
    "  x_test.append(feature)\n",
    "  y_test.append(label)\n",
    "\n",
    "# Normalize the data\n",
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer()\n",
    "x_train = norm.fit_transform(x_train)\n",
    "x_test = norm.transform(x_test)\n",
    "#x_train = np.array(x_train) / 255\n",
    "#x_test = np.array(x_test) / 255\n",
    "\n",
    "#x_train.reshape(-1, img_size, img_size, 1)\n",
    "#y_train = np.array(y_train)\n",
    "\n",
    "#x_test.reshape(-1, img_size, img_size, 1)\n",
    "#y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32cce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec530bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.000001)\n",
    "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['recall'])\n",
    "\n",
    "history = model.fit(x_train,y_train,epochs = 500 , validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(500)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
